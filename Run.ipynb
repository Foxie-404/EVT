{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Colab"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 找到上方菜单栏“代码执行程序”——>弹窗底部“更改运行时类型”，选择GPU作为硬件加速器"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EblwyDLicmnp"
      },
      "source": [
        "\n",
        "\n",
        "- 防止断开连接：按住 Ctrl+Shift 再按下 I 呼出弹窗，于控制台内输入以下内容并回车：\n",
        "```\n",
        "function ConnectButton()\n",
        "{\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "```\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mount Google Drive for Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Environment for Windows"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step1.（无Conda则直接执行第三步）"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "升级conda版本（可跳过）\n",
        "conda update -n base -c defaults conda\n",
        "\n",
        "创建虚拟环境（留意python版本是否支持）\n",
        "conda create -n My_Env python=3.9\n",
        "\n",
        "配置虚拟环境——安装ipython内核\n",
        "conda install -n My_Env ipykernel\n",
        "\n",
        "激活虚拟环境 (切换环境前记得deactivate)\n",
        "conda activate My_Env"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "切换Jupyter当前的内核到虚拟环境的内核"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 升级pip版本\n",
        "!python -m pip --default-timeout=900 install --upgrade pip\n",
        "# 安装pytorch（需从官网复制命令）\n",
        "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "# 安装项目依赖\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Environment for Linux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!apt-get update``\n",
        "!sudo apt install portaudio19-dev\n",
        "%cd drive/MyDrive/Easy Voice Toolkit\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup FFmpeg（Experimental）"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```已经安装了FFmpeg的用户可直接跳过```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 快速部署FFmpeg（实验性）\n",
        "!static_ffmpeg -i file.mp4 ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "如果报错，请手动安装FFmpeg"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%cd /content/drive/MyDrive/Easy Voice Toolkit\n",
        "\n",
        "import os\n",
        "from typing import Optional\n",
        "\n",
        "Dir_Current = '/content/drive/MyDrive/Easy Voice Toolkit'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tool_AudioProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(Dir_Current + '/Tool_AudioProcessor')\n",
        "\n",
        "from Process import Audio_Processing\n",
        "\n",
        "class Execute_Audio_Processing:\n",
        "    '''\n",
        "    Change media format to WAV and cut off the silent parts\n",
        "    '''\n",
        "    Media_Dir_Input: str = ''\n",
        "    Media_Dir_Output: str = '' #os.path.join(os.path.dirname(Media_Dir_Input), 'Output')\n",
        "    Media_Format_Output: str = 'wav'   # choices = ['mp3', 'wav', ...]\n",
        "    RMS_Threshold: float = -40.   # Default\n",
        "    Audio_Length_Min: int = 5000   # Default\n",
        "    Silent_Interval_Min: int = 300   # Default\n",
        "    Hop_Size: int = 10   # Default\n",
        "    Silence_Kept_Max: int = 1000   # Default\n",
        "\n",
        "    def Run():\n",
        "        AudioConvertandSlice = Audio_Processing(\n",
        "            Execute_Audio_Processing.Media_Dir_Input,\n",
        "            Execute_Audio_Processing.Media_Dir_Output,\n",
        "            Execute_Audio_Processing.Media_Format_Output,\n",
        "            Execute_Audio_Processing.RMS_Threshold, Execute_Audio_Processing.Audio_Length_Min,\n",
        "            Execute_Audio_Processing.Silent_Interval_Min, Execute_Audio_Processing.Hop_Size,\n",
        "            Execute_Audio_Processing.Silence_Kept_Max\n",
        "        )\n",
        "        AudioConvertandSlice.Convert_Media()\n",
        "        AudioConvertandSlice.Slice_Audio()\n",
        "\n",
        "Execute_Audio_Processing.Run()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tool_VoiceIdentifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(Dir_Current + '/Tool_VoiceIdentifier')\n",
        "\n",
        "from Identify import Voice_Identifying\n",
        "\n",
        "class Execute_Voice_Identifying:\n",
        "    '''\n",
        "    Contrast the voice and filter out the similar ones\n",
        "    '''\n",
        "    Audio_Path_Std: str = ''\n",
        "    Audio_Dir_Input: str = '' #Execute_Audio_Processing.Media_Dir_Output\n",
        "    Audio_Dir_Output: str = ''\n",
        "    Model_Dir: str = ''\n",
        "    Model_Type: str = 'Ecapa-Tdnn'   # choices = ['Ecapa-Tdnn']\n",
        "    Model_Name: str = 'small'   # choices = ['small']\n",
        "    Feature_Method: str = 'spectrogram'   # choices = ['spectrogram', 'melspectrogram']\n",
        "    DecisionThreshold: float = 0.84   # Recommanded\n",
        "    Duration_of_Audio: float = 4.20   # Recommanded\n",
        "\n",
        "    def Run():\n",
        "        AudioContrastInference = Voice_Identifying(\n",
        "            Execute_Voice_Identifying.Audio_Path_Std,\n",
        "            Execute_Voice_Identifying.Audio_Dir_Input,\n",
        "            Execute_Voice_Identifying.Audio_Dir_Output,\n",
        "            Execute_Voice_Identifying.Model_Dir,\n",
        "            Execute_Voice_Identifying.Model_Type,\n",
        "            Execute_Voice_Identifying.Model_Name,\n",
        "            Execute_Voice_Identifying.Feature_Method,\n",
        "            Execute_Voice_Identifying.DecisionThreshold,\n",
        "            Execute_Voice_Identifying.Duration_of_Audio\n",
        "        )\n",
        "        AudioContrastInference.GetModel()\n",
        "        AudioContrastInference.Inference()\n",
        "\n",
        "Execute_Voice_Identifying.Run()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tool_VoiceTranscriber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(Dir_Current + '/Tool_VoiceTranscriber')\n",
        "\n",
        "from Transcribe import Voice_Transcribing\n",
        "\n",
        "class Execute_Voice_Transcribing:\n",
        "    '''\n",
        "    Transcribe WAV content to SRT\n",
        "    '''\n",
        "    Model_Name: str = 'small'   # choices = ['tiny', 'base', 'small', 'medium', 'large']\n",
        "    Model_Dir: str = '' #Execute_Voice_Identifying.Model_Dir\n",
        "    WAV_Dir: str = '' #Execute_Voice_Identifying.Audio_Dir_Output\n",
        "    SRT_Dir: str = ''\n",
        "    Verbose: bool = True   # Default\n",
        "    Language: Optional[str] = None   # choices = ['zh', 'en', ..., None]\n",
        "    Condition_on_Previous_Text: bool = True   # Default\n",
        "\n",
        "    def Run():\n",
        "        WAVtoSRT = Voice_Transcribing(\n",
        "            Execute_Voice_Transcribing.Model_Name,\n",
        "            Execute_Voice_Transcribing.Model_Dir,\n",
        "            Execute_Voice_Transcribing.WAV_Dir,\n",
        "            Execute_Voice_Transcribing.SRT_Dir,\n",
        "            Execute_Voice_Transcribing.Verbose,\n",
        "            Execute_Voice_Transcribing.Language,\n",
        "            Execute_Voice_Transcribing.Condition_on_Previous_Text\n",
        "        )\n",
        "        WAVtoSRT.Transcriber()\n",
        "\n",
        "Execute_Voice_Transcribing.Run()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tool_DatasetCreator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(Dir_Current + '/Tool_DatasetCreator')\n",
        "\n",
        "from Create import Dataset_Creating\n",
        "\n",
        "class Execute_Dataset_Creating:\n",
        "    '''\n",
        "    Convert the whisper-generated SRT to CSV and split the WAV\n",
        "    '''\n",
        "    SRT_Dir: str = '' #Execute_Voice_Transcribing.SRT_Dir\n",
        "    WAV_Dir: str = '' #Execute_Voice_Transcribing.WAV_Dir\n",
        "    Sample_Rate: int = 22050   # For vits\n",
        "    Subtype: str = 'PCM_16'   # For vits\n",
        "    WAV_Dir_Split: str = ''\n",
        "    Encoder: str = 'VITS'   # choices = ['VITS']\n",
        "    IsSpeakerMultiple: bool = False   # Default\n",
        "    FileList_Path_Training: str = '' #'./FileLists/Train_FileList.txt'\n",
        "    FileList_Path_Validation: str = '' #'./FileLists/Val_FileList.txt'\n",
        "\n",
        "    def Run():\n",
        "        SRTtoCSVandSplitAudio = Dataset_Creating(\n",
        "            Execute_Dataset_Creating.SRT_Dir,\n",
        "            Execute_Dataset_Creating.WAV_Dir,\n",
        "            Execute_Dataset_Creating.Sample_Rate,\n",
        "            Execute_Dataset_Creating.Subtype,\n",
        "            Execute_Dataset_Creating.WAV_Dir_Split,\n",
        "            Execute_Dataset_Creating.Encoder,\n",
        "            Execute_Dataset_Creating.IsSpeakerMultiple,\n",
        "            Execute_Dataset_Creating.FileList_Path_Training,\n",
        "            Execute_Dataset_Creating.FileList_Path_Validation\n",
        "        )\n",
        "        SRTtoCSVandSplitAudio.CallingFunctions()\n",
        "\n",
        "Execute_Dataset_Creating.Run()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tool_VoiceEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(Dir_Current + '/Tool_VoiceEncoder')\n",
        "\n",
        "from Encode import Voice_Encoding\n",
        "\n",
        "class Execute_Voice_Encoding:\n",
        "    '''\n",
        "    Preprocess and then start training\n",
        "    '''\n",
        "    FileList_Path_Validation: str = '' #Execute_Dataset_Creating.FileList_Path_Validation\n",
        "    FileList_Path_Training: str = '' #Execute_Dataset_Creating.FileList_Path_Training\n",
        "    Language: str = 'chinese'   # Default\n",
        "    Config_Path_Load: Optional[str] = None   # choices = ['%CustomConfigPath%', None]\n",
        "    Config_Dir_Save: str = ''\n",
        "    Eval_Interval: int = 1000   # Recommanded\n",
        "    Epochs: int = 10000   # Recommanded\n",
        "    Batch_Size: int = 8   # choices = [Power of 2]\n",
        "    FP16_Run: bool = True   # Recommanded\n",
        "    IsSpeakerMultiple: str = '' #Execute_Dataset_Creating.IsSpeakerMultiple\n",
        "    N_Speakers: int = 0   # Default\n",
        "    Speakers: str = ''   # choices = ['%SpeakerName1%', '%SpeakerName2%', ...]\n",
        "    Num_Workers: int = 8   # Default\n",
        "    Model_Dir: str = ''\n",
        "    Model_Name: str = ''\n",
        "    \n",
        "    def Run():\n",
        "        PreprocessandTrain = Voice_Encoding(\n",
        "            Execute_Voice_Encoding.FileList_Path_Validation,\n",
        "            Execute_Voice_Encoding.FileList_Path_Training,\n",
        "            Execute_Voice_Encoding.Language,\n",
        "            Execute_Voice_Encoding.Config_Path_Load,\n",
        "            Execute_Voice_Encoding.Config_Dir_Save,\n",
        "            Execute_Voice_Encoding.Eval_Interval,\n",
        "            Execute_Voice_Encoding.Epochs,\n",
        "            Execute_Voice_Encoding.Batch_Size,\n",
        "            Execute_Voice_Encoding.FP16_Run,\n",
        "            Execute_Voice_Encoding.IsSpeakerMultiple,\n",
        "            Execute_Voice_Encoding.N_Speakers,\n",
        "            Execute_Voice_Encoding.Speakers,\n",
        "            Execute_Voice_Encoding.Num_Workers,\n",
        "            Execute_Voice_Encoding.Model_Dir,\n",
        "            Execute_Voice_Encoding.Model_Name\n",
        "        )\n",
        "        PreprocessandTrain. Preprocessing_and_Training()\n",
        "\n",
        "Execute_Voice_Encoding.Run()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "My_Env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "d638444b7179bdfc0dc1957817588c7007ff4b9946fa53f9cc2df304cc8f4127"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
